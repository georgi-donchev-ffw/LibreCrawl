"""Link management and extraction"""
import threading
from urllib.parse import urljoin, urlparse
from collections import deque


class LinkManager:
    """Manages link discovery, tracking, and extraction"""

    def __init__(self, base_domain):
        self.base_domain = base_domain
        self.visited_urls = set()
        self.discovered_urls = deque()
        self.all_discovered_urls = set()
        self.all_links = []
        self.links_set = set()
        self.source_pages = {}  # Maps target_url -> list of source_urls

        self.urls_lock = threading.Lock()
        self.links_lock = threading.Lock()

    def extract_links(self, soup, current_url, depth, should_crawl_callback):
        """Extract links from HTML and add to discovery queue"""
        links = soup.find_all('a', href=True)

        for link in links:
            href = link['href'].strip()
            if not href or href.startswith('#') or href.startswith('mailto:') or href.startswith('tel:'):
                continue

            # Convert relative URLs to absolute
            absolute_url = urljoin(current_url, href)

            # Clean URL (remove fragment)
            parsed = urlparse(absolute_url)
            clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
            if parsed.query:
                clean_url += f"?{parsed.query}"

            # Thread-safe checking and adding
            with self.urls_lock:
                # Track source page for this URL
                if clean_url not in self.source_pages:
                    self.source_pages[clean_url] = []
                if current_url not in self.source_pages[clean_url]:
                    self.source_pages[clean_url].append(current_url)

                if (clean_url not in self.visited_urls and
                    clean_url not in self.all_discovered_urls and
                    clean_url != current_url):

                    # Check if this URL should be crawled
                    if should_crawl_callback(clean_url, depth):
                        self.all_discovered_urls.add(clean_url)
                        self.discovered_urls.append((clean_url, depth))

    def collect_all_links(self, soup, source_url, crawl_results):
        """Collect all links for the Links tab display"""
        links = soup.find_all('a', href=True)

        for link in links:
            href = link['href'].strip()
            if not href or href.startswith('#'):
                continue

            # Get anchor text
            anchor_text = link.get_text().strip()[:100]

            # Handle special link types
            if href.startswith('mailto:') or href.startswith('tel:'):
                continue

            # Convert relative URLs to absolute
            try:
                absolute_url = urljoin(source_url, href)
                parsed_target = urlparse(absolute_url)

                # Clean URL (remove fragment)
                clean_url = f"{parsed_target.scheme}://{parsed_target.netloc}{parsed_target.path}"
                if parsed_target.query:
                    clean_url += f"?{parsed_target.query}"

                # Determine if link is internal or external
                target_domain = parsed_target.netloc.lower()
                base_domain = self.base_domain.lower() if self.base_domain else ''
                
                # Remove 'www.' prefix if present at the start
                target_domain_clean = target_domain[4:] if target_domain.startswith('www.') else target_domain
                base_domain_clean = base_domain[4:] if base_domain.startswith('www.') else base_domain
                
                # Only consider internal if both domains are non-empty and match
                is_internal = (target_domain_clean and base_domain_clean and 
                              target_domain_clean == base_domain_clean)

                # Find the status of the target URL if we've crawled it
                target_status = None
                for result in crawl_results:
                    if result['url'] == clean_url:
                        target_status = result['status_code']
                        break

                # Determine placement (navigation, footer, body)
                placement = self._detect_link_placement(link)
                
                # Generate DOM path (XPath-like)
                link_path = self._get_dom_path(link)

                link_data = {
                    'source_url': source_url,
                    'target_url': clean_url,
                    'anchor_text': anchor_text or '(no text)',
                    'is_internal': is_internal,
                    'target_domain': parsed_target.netloc,
                    'target_status': target_status,
                    'placement': placement,
                    'link_path': link_path
                }

                # Track source page for this URL (for "Linked From" feature)
                with self.urls_lock:
                    if clean_url not in self.source_pages:
                        self.source_pages[clean_url] = []
                    if source_url not in self.source_pages[clean_url]:
                        self.source_pages[clean_url].append(source_url)

                # Thread-safe adding to links collection with duplicate checking
                with self.links_lock:
                    link_key = f"{link_data['source_url']}|{link_data['target_url']}"

                    if link_key not in self.links_set:
                        self.links_set.add(link_key)
                        self.all_links.append(link_data)

            except Exception:
                continue

    def _detect_link_placement(self, link_element):
        """Detect where on the page a link is placed"""
        # Check parent elements up the tree
        current = link_element.parent

        while current and current.name:
            # Check for footer
            if current.name == 'footer':
                return 'footer'

            # Check for footer by class/id
            classes = current.get('class', [])
            element_id = current.get('id', '')
            classes_str = ' '.join(classes).lower() if classes else ''

            if 'footer' in classes_str or 'footer' in element_id.lower():
                return 'footer'

            # Check for navigation
            if current.name in ['nav', 'header']:
                return 'navigation'

            # Check for navigation by class/id
            if any(keyword in classes_str or keyword in element_id.lower()
                   for keyword in ['nav', 'menu', 'header']):
                return 'navigation'

            current = current.parent

        # Default to body if not in nav or footer
        return 'body'

    def _get_dom_path(self, element):
        """Generate XPath-like DOM path for an element (similar to Screaming Frog format)"""
        if not element or not element.name:
            return ''
        
        path_parts = []
        current = element
        
        # Walk up the tree to build path
        while current and current.name:
            tag_name = current.name.lower()
            
            # Skip html tag, start from body
            if tag_name == 'html':
                break
            
            # Build path segment
            segment = tag_name
            
            # Add id if available (most specific)
            element_id = current.get('id')
            if element_id:
                segment += f"[@id='{element_id}']"
            else:
                # Count siblings with same tag name for index
                parent = current.parent
                if parent:
                    # Filter to only element siblings (not text nodes)
                    siblings = [sib for sib in parent.children 
                               if hasattr(sib, 'name') and sib.name and sib.name == tag_name]
                    if len(siblings) > 1:
                        try:
                            index = siblings.index(current) + 1
                            segment += f"[{index}]"
                        except ValueError:
                            # Current not in siblings list (shouldn't happen, but handle gracefully)
                            pass
            
            path_parts.insert(0, segment)
            current = current.parent
            
            # Stop at body (Screaming Frog style starts from body)
            if tag_name == 'body':
                break
        
        # Join with / separator, starting with //body
        if path_parts:
            return '//' + '/'.join(path_parts)
        return ''

    def is_internal(self, url):
        """Check if URL is internal to the base domain"""
        parsed_url = urlparse(url)
        
        url_domain = parsed_url.netloc.lower()
        base_domain = self.base_domain.lower() if self.base_domain else ''
        
        # Remove 'www.' prefix if present at the start
        url_domain_clean = url_domain[4:] if url_domain.startswith('www.') else url_domain
        base_domain_clean = base_domain[4:] if base_domain.startswith('www.') else base_domain
        
        # Only consider internal if both domains are non-empty and match
        return (url_domain_clean and base_domain_clean and 
                url_domain_clean == base_domain_clean)

    def add_url(self, url, depth):
        """Add a URL to the discovery queue"""
        with self.urls_lock:
            if url not in self.all_discovered_urls and url not in self.visited_urls:
                self.all_discovered_urls.add(url)
                self.discovered_urls.append((url, depth))

    def mark_visited(self, url):
        """Mark a URL as visited"""
        with self.urls_lock:
            self.visited_urls.add(url)

    def get_next_url(self):
        """Get the next URL to crawl"""
        with self.urls_lock:
            if self.discovered_urls:
                return self.discovered_urls.popleft()
        return None

    def get_stats(self):
        """Get current statistics"""
        with self.urls_lock:
            return {
                'discovered': len(self.all_discovered_urls),
                'visited': len(self.visited_urls),
                'pending': len(self.discovered_urls)
            }

    def update_link_statuses(self, crawl_results):
        """Update target_status for all links based on crawl results"""
        # Build a fast lookup dict
        status_lookup = {result['url']: result['status_code'] for result in crawl_results}

        with self.links_lock:
            for link in self.all_links:
                target_url = link['target_url']
                if target_url in status_lookup:
                    link['target_status'] = status_lookup[target_url]

    def get_source_pages(self, url):
        """Get list of source pages that link to this URL"""
        with self.urls_lock:
            return self.source_pages.get(url, []).copy()

    def reset(self):
        """Reset all state"""
        with self.urls_lock:
            self.visited_urls.clear()
            self.discovered_urls.clear()
            self.all_discovered_urls.clear()
            self.source_pages.clear()

        with self.links_lock:
            self.all_links.clear()
            self.links_set.clear()
